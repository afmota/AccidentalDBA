<!DOCTYPE html>
<html lang="pt-br">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>DBA Acidental</title>
        <link rel="stylesheet" href="estilo/style.css">
        <link rel="stylesheet" href="estilo/media-queries.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
    </head>
    <body>
        <header>
            <h1>Troubleshooting SQL Server</h1>
            <h2>Um Guia para o DBA Acidental</h2>
        </header>

        <nav>
            <i id="burger" class="material-icons" onclick="clickMenu()">menu</i>
            <menu id="itens">
                <ul>
                    <li><a href="index.html" target="_self"><i id="home" class="material-icons">home</i></a></li>
                    <li><a href="abertura.html" target="_self">Abertura</a></li>
                    <li><a href="introducao.html" target="_self">Introdução</a></li>
                    <li><a href="cap01.html" target="_self">Cap.1</a></li>
                    <li><a href="cap02.html" target="_self">Cap.2</a></li>
                    <li><a href="cap03.html" target="_self">Cap.3</a></li>
                    <li><a href="cap04.html" target="_self">Cap.4</a></li>
                    <li><a href="cap05.html" target="_self">Cap.5</a></li>
                    <li><a href="cap06.html" target="_self">Cap.6</a></li>
                    <li><a href="cap07.html" target="_self">Cap.7</a></li>
                </ul>
            </menu>
        </nav>

        <main>
            <h1>Capítulo 8: Log de Transações Grande ou Completo</h1>

            <p>Um log de transações é um arquivo físico no qual o SQL Server armazena um registro de todas as transações e modificações de dados realizadas no banco de dados com o qual o arquivo de log está associado. É, indiscutivelmente, o componente mais importante de um banco de dados SQL Server, uma vez que o mecanismo do SQL Server o utiliza para garantir a durabilidade da transação (todos os dados válidos e confirmados serão preservados) e o rollback da transação (os efeitos de quaisquer transações parciais não confirmadas, no arquivo de dados, podem ser "desfeitos"), e o DBA do SQL Server pode utilizá-lo para restaurar um banco de dados a um ponto anterior no tempo, em caso de desastre.</p>

            <p>Infelizmente, no entanto, é também um dos componentes mais incompreendidos e mal geridos do SQL Server e os problemas resultantes são uma fonte frequente de perguntas e pedidos de ajuda nos fóruns técnicos online. Este capítulo examinará, brevemente, como o log de transações funciona e, em seguida, discutirá os problemas mais comuns e as formas de má gestão que levam ao crescimento descontrolado do log de transações, incluindo:</p>

            <ul>
                <li>executar a manutenção de índices;</li>
                <li>operar um banco de dados no modo de recuperação <code>FULL</code>, sem fazer backups do log;</li>
                <li>transações de longa duração ou não confirmadas que impedem o reuso do espaço no log de transações.</li>
            </ul>

            <p>Claro, se o crescimento descontrolado não for controlado, seu arquivo de log pode se expandir até consumir todo o espaço do disco, momento em que você receberá o infame erro 9002 (log de transações cheio), e o banco de dados se tornará somente leitura. Vamos cobrir as maneiras corretas de responder ao crescimento descontrolado do log e ao erro 9002, e também explicar por que o conselho comumente dado para truncar o log e encolhê-lo é muitas vezes perigoso.</p>

            <p>Por fim, abordaremos estratégias para garantir um crescimento suave e previsível do seu arquivo de log, enquanto minimizamos os problemas associados à fragmentação do log. Em um banco de dados movimentado, um grande log de transações pode ser um simples fato da vida e, se gerido corretamente, isso não é necessariamente algo ruim, mesmo que o espaço do arquivo de log seja inutilizado a maior parte do tempo.</p>

            <p>Observe que, para focar no tópico de diagnosticar e resolver um log de transações descontrolado ou cheio, assumimos conhecimento prévio de tópicos relacionados, como modelos de recuperação de banco de dados. Se precisar de uma revisão, uma boa fonte é o artigo "Modelos de Recuperação" de Gail Shaw, disponível em <a href="http://www.sqlservercentral.com/articles/Administration/75461/" target="_blank">http://www.sqlservercentral.com/articles/Administration/75461/</a>.</p>

            <h2 id="logtransactionworks">Como o Log de Transações Trabalha</h2>

            <p>Sempre que uma alteração é feita em um objeto de banco de dados ou nos dados que ele contém, não apenas os dados ou o objeto são atualizados no arquivo de dados, mas também os detalhes da alteração são registrados como uma sequência de registros de log no log de transações. Cada registro de log contém os detalhes de uma mudança específica que foi feita no banco de dados, permitindo que essa mudança seja executada novamente como parte do <code>REDO</code>, ou desfeita como parte do <code>UNDO</code>, durante a recuperação de falhas. Outros registros de log contêm detalhes sobre o ID da transação que realizou a alteração, quando essa transação começou e terminou, quais páginas foram alteradas, as mudanças de dados que foram feitas e assim por diante.</p>

            <p>O SQL Server, como a maioria dos sistemas de banco de dados relacionais transacionais, utiliza o protocolo de Registro Antecipado de Escrita (Write-Ahead Logging - WAL) para todas as modificações de dados que ocorrem em todos os bancos de dados.</p>

            <div class="dica">
                <p class="destaque">The WAL protocol</p>
                <p>This is described by Chunder Mohan in the paper, "ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging" (<a href="http://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf" target="_blank">http://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf</a>).</p>
            </div>

            <p>O protocolo WAL (Write-Ahead Logging) dita que, antes de uma página de dados ser alterada em armazenamento não volátil, as informações descrevendo a mudança na página de dados devem primeiro ser gravadas em armazenamento estável, permitindo que a alteração seja refeita ou desfeita em caso de falha. O SQL Server implementa este protocolo registrando as informações que descrevem mudanças em objetos de banco de dados e páginas de dados usando registros de log no log de transações. Antes de uma transação que altera dados no banco de dados ser concluída, os registros de log que descrevem as alterações devem primeiro ser gravados no disco. As alterações feitas nas páginas de dados no cache de buffer são então subsequentemente gravadas em armazenamento permanente nos arquivos de banco de dados no ponto de verificação, ou pelo processo de Escritor Preguiçoso (Lazy Writer) quando a instância está sob pressão de memória no pool de buffers. Alterações não confirmadas também podem ser gravadas no arquivo de dados pelos processos de <code>CHECKPOINT</code> ou Lazy Writer, desde que os registros de log que descrevem essas alterações tenham sido primeiro escritos no arquivo de log de transações.</p>

            <p>Ao sempre gravar as alterações no arquivo de log primeiro, o SQL Server tem a base para um mecanismo que pode garantir que os efeitos de todas as transações confirmadas serão refletidos nos arquivos de dados, e que quaisquer modificações de dados no disco originadas de transações incompletas, ou seja, aquelas para as quais nem um <code>COMMIT</code> nem um <code>ROLLBACK</code> foram emitidos, não serão refletidas nos arquivos de dados.</p>

            <p>Este processo de conciliação do conteúdo dos arquivos de dados e log ocorre durante o <strong>processo de recuperação do banco de dados</strong> (às vezes chamado de recuperação após falha), que é iniciado automaticamente sempre que o SQL Server é reiniciado, ou como parte do comando <code>RESTORE</code>.</p>

            <p>Quaisquer transações que foram confirmadas e, portanto, registradas no log de transações, antes da interrupção do serviço, ou antes do tempo para o qual o banco de dados está sendo restaurado manualmente, mas que não estão refletidas nos arquivos de dados, serão "avançadas" (refeitas). Da mesma forma, quaisquer alterações de dados no banco de dados associadas a transações não confirmadas serão "desfeitas", lendo as operações relevantes do arquivo de log e executando a operação física reversa nos dados.</p>

            <p>Dessa forma, o SQL Server garante que todas as ações associadas a uma transação sejam bem-sucedidas como uma unidade, ou que todas falhem, permitindo que o SQL Server retorne o banco de dados a um estado consistente em relação a um ponto específico no tempo. Como tal, o log de transações representa um dos meios fundamentais pelos quais o SQL Server assegura a consistência e integridade dos dados durante a operação normal.</p>

            <h3 id="comoescreve">Como o SQL Server escreve no log de transações</h3>

            <p>A maneira como o SQL Server escreve no log de transações é basicamente diferente da maneira como ele escreve nos arquivos de dados. As escritas nos <strong>arquivos de dados</strong> ocorrem de forma aleatória, uma vez que as alterações nos dados afetam páginas aleatórias armazenadas no banco de dados. Como tal, o disco no qual um arquivo de dados é armazenado é regularmente submetido a múltiplas operações de busca, que reposicionam a cabeça para o cilindro de disco correto a fim de gravar as alterações. O desempenho da escrita pode ser melhorado escrevendo-se nos arquivos de dados de forma intercalada utilizando uma metodologia de preenchimento proporcional, onde a quantidade de dados escrita em um arquivo específico é proporcional à quantidade de espaço livre no arquivo, em comparação com a quantidade de espaço livre em outros arquivos no grupo de arquivos.</p>

            <p>Por outro lado, o SQL Server escreve no <strong>log de transações</strong> de forma sequencial, um registro após o outro, e, portanto, um disco que armazena apenas um log de transações raramente precisará realizar operações de busca aleatórias. Certas operações, como backups de log de transações, tarefas de Captura de Dados de Alteração (Change Data Capture) e o agente leitor de log de replicação, lerão o log de transações de forma "aleatória", mas as operações de escrita são geralmente sequenciais e podem ser muito mais rápidas. É por isso que a prática recomendada é separar o arquivo de log dos arquivos de dados e armazenar o primeiro em discos físicos configurados para gravações sequenciais de alta velocidade, conforme discutido no <a href="cap02.html" target="_self">Capítulo 2, Configuração de E/S de Disco</a>.</p>

            <p>O fato de que as escritas são sempre sequenciais também significa que o SQL Server sempre escreverá em apenas um arquivo de log de transações por vez. Portanto, não há vantagem, em termos de desempenho de escrita do log, em ter múltiplos arquivos de log de transações. A única razão para ter mais de um arquivo de log é se as considerações de espaço exigirem a necessidade de múltiplos arquivos de log, em discos diferentes, para alcançar o tamanho necessário do log para um banco de dados.</p>
            
            <h3 id="logtruncation">Compreendendo a truncagem do log</h3>

            <p>A truncagem do log é o mecanismo pelo qual o SQL Server marca o espaço dentro do log de transações como disponível para reutilização pelo banco de dados. O espaço alocado dentro de um arquivo de log de transações é internamente dividido em segmentos menores conhecidos como arquivos de log virtual (VLFs), e o processo de truncagem do log é simplesmente o ato de marcar um VLF como "inativo", tornando o espaço nesse VLF disponível para reutilização. Isso não reduz o tamanho físico do log de transações, como o termo "truncagem" poderia sugerir.</p>

            <p>Um VLF só pode ser considerado inativo se não contiver nenhuma parte do que é denominado log ativo. Uma discussão completa sobre a arquitetura do log de transações está fora do escopo, mas, brevemente, qualquer registro de log relacionado a uma transação aberta é necessário para um possível rollback e, portanto, deve fazer parte do log ativo. Além disso, há várias outras atividades no banco de dados, incluindo replicação, espelhamento e CDC (Captura de Dados de Alteração), que usam o log de transações e precisam que os registros do log de transações permaneçam disponíveis até serem processados. Esses registros também farão parte do log ativo.</p>

            <p>Como discutido anteriormente, os arquivos de log de transações são arquivos <strong>sequenciais</strong> e cada registro de log inserido no arquivo de log é marcado com um <strong>Número Sequencial Lógico</strong> (<strong>Logical Sequence Number - LSN</strong>). O registro de log com o menor LSN (MinLSN) é definido como o registro de log mais antigo que ainda pode ser necessário para alguma operação ou atividade do banco de dados, e este registro marca o início do log ativo. O registro de log com o maior LSN (ou seja, o registro mais recente adicionado) marca o fim do log ativo.</p>

            <p>Um registro de log não faz mais parte do log ativo se cada uma das seguintes três condições for atendida:</p>

            <ol>
                <li>Ele está relacionado a uma transação que foi confirmada e, portanto, não é mais necessária para rollback.</li>
                <li>Ele não é mais necessário para nenhum outro processo de banco de dados, incluindo um backup de log de transações ao usar modelos de recuperação <code>FULL</code> ou <code>BULK LOGGED</code>.</li>
                <li>Ele é mais antigo (ou seja, tem um LSN menor) do que o registro MinLSN.</li>
            </ol>

            <p>Qualquer VLF que não contenha parte do log ativo está inativo e pode ser truncado, embora o ponto em que essa truncagem ocorre dependa do modelo de recuperação em uso. No modelo de recuperação <code>SIMPLE</code>, a truncagem pode ocorrer imediatamente após o <code>CHECKPOINT</code>; páginas de dados em cache são gravadas no disco (após a gravação dos detalhes da transação) e quaisquer VLFs que não contenham parte do log ativo são truncados.</p>

            <p>No modelo de recuperação <code>FULL</code> (ou <code>BULK LOGGED</code>), uma vez que um backup completo do banco de dados tenha sido realizado, a parte inativa do log não é marcada como reutilizável no <code>CHECKPOINT</code>, porque é necessário manter uma cadeia de LSN completa, e assim permitir a recuperação do banco de dados em um ponto no tempo. A truncagem só pode ocorrer em uma operação de <code>BACKUP LOG</code>. Nesse caso, uma vez que o backup do log tenha sido realizado, ele marca quaisquer VLFs que não sejam mais necessários como inativos e, portanto, reutilizáveis.</p>

            <p>Mais adiante no capítulo, na seção intitulada <a href="">Falta de reutilização do espaço do log</a>, discutiremos fatores, como transações não confirmadas ou de longa duração, que podem impedir a reutilização do espaço e fazer com que o arquivo de log cresça rapidamente em tamanho.</p>

            <h3 id="sizinglog">Dimensionando e aumentando o log</h3>

            <p>Sempre que um arquivo de log precisa crescer e um espaço adicional é alocado, esse espaço é dividido igualmente em VLFs, com base na quantidade de espaço que está sendo alocada. Quando o espaço adicional é alocado em blocos pequenos, por exemplo, usando uma configuração de crescimento automático de dez por cento por padrão, o log de transações resultante pode ter um grande número de pequenos VLFs. Quando o espaço adicional é alocado em tamanhos maiores, por exemplo, ao dimensionar inicialmente o log para 16 GB em uma única operação, o log de transações resultante tem um pequeno número de VLFs maiores.</p>

            <div class="dica">
                <p class="destaque">VLFs de Log de Transações – muitos ou poucos?</p>
                A MVP do SQL Server, Kimberly Tripp, discute o impacto dos tamanhos dos VLFs e fornece orientações sobre como gerenciar adequadamente o tamanho dos VLFs em sua postagem no blog, "VLFs de Log de Transações – muitos ou poucos?" (<a href="http://www.sqlskills.com/BLOGS/KIMBERLY/post/Transaction-Log-VLFs-too-many-or-too-few.aspx" target="_blank">http://www.sqlskills.com/BLOGS/KIMBERLY/post/Transaction-Log-VLFs-too-many-or-too-few.aspx</a>).
            </div>

            <p>Um número muito alto de pequenos VLFs, conhecido como fragmentação de arquivo de log, pode ter um impacto considerável no desempenho, especialmente para recuperação após falhas, restaurações e backups, particularmente backups de log; em outras palavras, operações que leem o arquivo de log. Por outro lado, se o banco de dados tiver apenas alguns VLFs grandes, isso pode levar a problemas relacionados ao rápido crescimento do log em casos em que a truncagem é atrasada, por algum motivo (veja a seção <a href="cap08.html#">Falta de reutilização do espaço do log</a>). Por exemplo, vamos supor que cada VLF tenha 1 GB de tamanho. Se todos os VLFs no arquivo de log contiverem alguma parte do log ativo, então o arquivo de log crescerá em passos de 1 GB até que alguns dos VLFs existentes deixem de conter qualquer parte do log ativo, momento em que poderá ser truncado pelo próximo backup de log se estiver usando recuperação <code>FULL</code>, ou pela próxima operação de <code>CHECKPOINT</code> na recuperação <code>SIMPLE</code>. Portanto, é importante que o log seja dimensionado adequadamente inicialmente e cresça em etapas de tamanho apropriado, para minimizar a fragmentação, mas também evitar a alocação de grandes porções do log por longos períodos.</p>

            <p>Há também uma segunda razão pela qual é muito importante dimensionar o log de forma adequada e aumentá-lo de maneira muito controlada: para arquivos de log, cada evento de crescimento é uma operação relativamente cara. É natural que tanto os arquivos de dados quanto os arquivos de log cresçam de tamanho ao longo do tempo, mas enquanto o processo de adicionar novos arquivos de dados e expandir os arquivos de dados existentes pode ser otimizado até certo ponto, permitindo a <strong>inicialização instantânea de arquivos</strong>, o mesmo não é verdadeiro para arquivos de log. Quando um arquivo de dados é criado ou cresce, o espaço alocado no disco precisa ser inicializado, ou preenchido com zeros, pelo SQL Server para remover os vestígios de qualquer dado anterior. O SQL Server 2005 introduziu um novo recurso, a inicialização instantânea de arquivos, para permitir que os arquivos de dados alocassem espaço no disco sem precisar preencher o espaço com zeros. No entanto, esse recurso não se aplica a arquivos de log, que ainda exigem inicialização e preenchimento com zeros sempre que o espaço é alocado.</p>
            
            <p>O log de transações, quando gerido corretamente, funciona de maneira circular, e o ponto de início das transações que devem ser processadas como parte da recuperação após falhas é mantido na página de inicialização do banco de dados. No entanto, nada rastreia a posição do último registro de log que requer processamento durante a recuperação após falhas, então os registros de log são marcados com um bit de paridade que é invertido quando o log de transações atinge o final do arquivo e volta ao início do arquivo. Para evitar a possibilidade de introduzir corrupção processando dados aleatórios que existiam anteriormente no disco e que coincidem com o bit de paridade dos registros de log existentes, o espaço alocado deve ser preenchido com zeros sempre que o arquivo de log cresce.</p>

            <div class="dica">
                <p class="destaque">Por que o log de transações não pode usar a inicialização instantânea?</p>
                <p>Para mais informações sobre a inicialização do log de transações, veja a postagem no blog de Paul Randal, "Search Engine Q&A#24: Why can't the transaction log use instant initialization?" (http://sqlskills.com/BLOGS/PAUL/post/Search-Engine-QA-24-Why-cant-the-transaction-log-use-instant-initialization.aspx).</p>
            </div>

            <h2 id="runawaylog">Diagnóstico de um Log de Transações Fora de Controle</h2>

            If you are experiencing uncontrolled growth of the transaction log, it is due either to an
            incredibly high rate of log activity, or to factors that are preventing space in the log file
            from being reused, or both.
            If the growth is due primarily to excessive log activity, you need to investigate whether
            there might be log activity that could be avoided, for example, by adjusting how you carry
            out bulk data and index maintenance operations, so that these operations are not fully
            logged (i.e. the BULK LOGGED recovery model is used for these operations). However,
            any bulk logged operation will immediately prevent point-in-time recovery to any point
            within a log file that contains records relating to the minimally logged operations. If this
            is not acceptable, you must simply accept a large log as a fact, and plan its growth and281
            Chapter 8: Large or Full Transaction Log
            management (such as frequency of log backups) accordingly, as described in the Proper
            Log Management section later in this chapter.
            If the growth is due to a lack of log space reuse, you need to find out what is preventing
            this reuse and take steps to correct the issue.
            Excessive logging: index maintenance operations
            Index maintenance operations are the second most common cause of transaction log
            usage and growth, especially in databases using the FULL recovery model. The amount of
            log space required to perform index maintenance depends on the following factors:
            •	 rebuild or reorganize – index rebuilds generally use a lot more space in the log
            •	 recovery model – if the risks to point-in-time recovery are understood and acceptable,
            then index rebuilds can be minimally logged by temporarily switching the database
            to run in BULK LOGGED recovery mode. Index reorganization, however, is always
            fully logged.
            Index rebuilds
            Rebuilding an index offline, using ALTER INDEX REBUILD (or the deprecated
            DBCC DBREINDEX in SQL Server 2000) drops the target index and rebuilds it from
            scratch (online index rebuilds do not drop the existing index until the end of the
            rebuild operation).
            Logging and online index rebuilds
            Online Index Rebuild is a fully-logged operation on SQL Server 2008 and later, whereas it is minimally
            logged in SQL Server 2005. Therefore, performing such operations in later SQL Server versions will
            require substantially more transaction log space. See: http://support.microsoft.com/kb/2407439.282
            Chapter 8: Large or Full Transaction Log
            In the FULL recovery model, index rebuilds can be a very resource intensive operation,
            requiring a lot of space in the transaction log. In the SIMPLE or BULK LOGGED recovery
            model, rebuilding an index is a minimally logged operation, meaning that only the allocations are logged, and not the actual pages changed, therefore reducing the amount of log
            space required by the operation.
            If you switch to the SIMPLE model to perform an index rebuild, the LSN chain will
            be automatically broken. You'll only be able to recover your database to a point of
            time contained in the previous transaction log backup. To restart the chain, you'll
            need to switch back to the FULL model and immediately take a full or differential
            database backup.
            If you switch to the BULK LOGGED model, the LSN chain is always maintained but there
            are still implications for your ability to perform point-in-time restores, since a log backup
            that contains a minimally logged operation can't be used to recover to a point in time. In
            other words, you won't be able to use the STOPAT option when restoring a log file that
            contains minimally logged operations. It is still possible to restore the entire transaction
            log backup to roll the database forward, and it is still possible to restore to a point in
            time in a subsequent log file, which doesn't contain any minimally logged operations.
            However, in the event of an application bug, or a user change that causes data to be
            deleted, around the same period as the minimally logged operation, it will not be possible
            to stop at a specific point in time in the log in which these changes are recorded, in order
            to recover the data that was deleted.
            If the ability to perform a point-in-time recovery is paramount for a database, the BULK
            LOGGED recovery model should not be used for index rebuilds or any other minimally
            logged operation, unless it can be done at a time when there is no concurrent user activity
            in the database.
            If the BULK LOGGED model is used, steps should be taken to minimize the time period
            where point-in-time restore is unavailable, and so minimize exposure to data loss. To do
            this, take a log backup in FULL mode, switch to BULK LOGGED, perform the index rebuild,
            then switch back to FULL and take another log backup.283
            Chapter 8: Large or Full Transaction Log
            A final important point to note is that an ALTER INDEX REBUILD operation occurs in a
            single transaction. If the index is large, this could represent a long-running transaction
            that will prevent space reuse in the log for its duration. This means that, even if you
            rebuild an index in SIMPLE mode, where you might think that the log should remain
            small since it is auto-truncated during a checkpoint operation, the log file can expand
            rapidly during the operation.
            Index reorganization
            In contrast to rebuilding an index, reorganizing (defragmenting) an index, using ALTER
            INDEX REORGANIZE (or the deprecated DBCC INDEXDEFRAG in SQL Server 2000) is
            always a fully-logged operation, regardless of the recovery model, and so the actual
            page changes are always logged. However, index reorganizations generally require less
            log space than an index rebuild, although this is a function of the amount of fragmentation that exists in the index; a heavily fragmented index will require more log space to
            reorganize than a minimally fragmented one.
            Furthermore, the ALTER INDEX REORGANIZE operation is accomplished using multiple,
            shorter transactions. Therefore, when performed in conjunction with frequent log
            backups (or when working in SIMPLE mode), log space can be made available for reuse
            during the operation, so minimizing the size requirements for the transaction log during
            the operation.
            For example, rebuilding a 20 GB index can require more than 20 GB of space for the
            rebuild operation because it occurs in a single transaction. However, reorganizing a 20 GB
            index requires much less log space because each page allocation change in the reorganization is a separate transaction, and so the log records can be truncated with frequent log
            backups, allowing the log space to be reused.284
            Chapter 8: Large or Full Transaction Log
            Strategies for controlling excessive logging
            If your organization has zero tolerance to any potential data loss, then you'll have no
            choice but to perform index rebuild operations in the FULL recovery model, and plan
            your log size and growth appropriately. If your Service Level Agreements (SLAs) and
            Operational Level Agreements (OLAs) allow some potential for data loss, then switching
            to BULK LOGGED recovery at the start of index rebuild can minimize the amount of space
            required to rebuild the index. However, do so in a way that minimizes exposure to data
            loss, as discussed earlier.
            If your database is in FULL recovery, and is subject to frequent index reorganization
            operations, then you might need to consider increasing the frequency with which you
            take log backups, especially during the time that the indexes are being rebuilt, in order to
            control the size of the log. Regardless of the frequency of log backups, the log will be at
            least as large as the largest index being rebuilt.
            Regardless of the recovery model in use, one can minimize the impact of index maintenance operations on the transaction log by reorganizing rather than rebuilding, if
            possible, and by only maintaining those indexes that really need it.
            Microsoft has provided guidelines that can be used for most, but not all, environments
            for determining when to rebuild an index versus when to reorganize it to minimize the
            impact of index maintenance operations. These guidelines can be found in the Books
            Online Topic, Reorganizing and Rebuilding Indexes (http://technet.microsoft.com/
            en-us/library/ms189858.aspx). They state that for fragmentation levels greater than 5
            percent but less than or equal to 30 percent, you should reorganize the index, and for
            fragmentation levels greater than 30 percent, you should rebuild it.
            It's also worth noting that rebuilding small indexes is generally not worthwhile. The
            commonly cited threshold is around 1,000 pages. These values are based on recommendations made by Paul Randal while he managed the storage engine development team
            at Microsoft, and which are documented in Books Online. Note, though, that this is
            guideline advice only and may not be appropriate for all environments, as discussed by285
            Chapter 8: Large or Full Transaction Log
            Paul in his blog post, Where do the Books Online index fragmentation thresholds come from?
            (http://www.sqlskills.com/BLOGS/PAUL/post/Where-do-the-Books-Online-indexfragmentation-thresholds-come-from.aspx).
            If you use the SSMS Maintenance Plans Wizard for index maintenance, it is an all-ornothing process: you either rebuild or reorganize all indexes in your database (and all
            databases in the maintenance plan) or you rebuild none of them. A better approach may
            be to use the sys.dm_db_index_physical_stats DMV to investigate fragmentation
            and so determine a rebuild/reorganize strategy based on need.
            Ola Hallengren's free maintenance scripts
            A comprehensive set of free maintenance scripts is made available online by Ola Hallengren. Ola's scripts
            demonstrate how to use sys.dm_db_index_physical_stats to perform index analysis for intelligent maintenance, and can be used as a replacement for Database Maintenance Plans created by the
            wizards in SSMS (http://ola.hallengren.com).
            Lack of log space reuse
            If you suspect log growth is being caused by the log space not being reused, your first
            job is to find out what's preventing reuse. Start by querying sys.databases, as shown
            in Listing 8.1, and see what the value of the column log_reuse_wait_desc is for the
            database mentioned in the error message.
            DECLARE @DatabaseName VARCHAR(50) ;
            SET @DatabaseName = 'VeryImportant'
            SELECT name ,
            recovery_model_desc ,
            log_reuse_wait_desc
            FROM sys.databases
            WHERE name = @DatabaseName
            Listing 8.1: Examining the value of the log_reuse_wait_desc column.286
            Chapter 8: Large or Full Transaction Log
            The value of the log_reuse_wait_desc column will show the current reason why log
            space cannot be reused. It is possible more than one thing is preventing log reuse. The
            sys.databases view will only show one of the reasons. It is therefore possible to resolve
            one problem, query sys.databases again and see a different log_reuse_wait reason.
            The possible values for log_reuse_wait_desc are listed in Books Online (http://
            msdn.microsoft.com/en-us/library/ms178534.aspx), but we'll cover the most common
            causes here, and explain how to safely ensure that space can start to get reused.
            FULL recovery model without log backups
            If the value returned for log_reuse_wait_desc, from the previous sys.databases
            query, is Log Backup, then you are suffering from one of the most common causes of
            a full or large transaction log, namely operating a database in the FULL recovery model
            (or less common, but still possible, the BULK_LOGGED recovery model), without taking
            transaction log backups.
            It varies, depending on the edition of SQL Server that is installed, but the model database
            is configured in FULL recovery mode at installation, for many editions. Since the model
            database is a template database that is used to create new databases in SQL Server, this
            configuration is inherited from model, by the new database.
            Using the FULL recovery model is a recommended practice for most production database
            environments, since it allows for point-in-time recovery of the database, minimizing data
            loss in the event of a disaster. However, a common mistake is then to adopt a backup
            strategy consisting entirely of full (and possibly differential) database backups without
            taking frequent transaction log backups. There are two big problems with this strategy:
            1. Taking full database backups only protects the contents of the data file, not the log
            file. The only way to properly protect the data that has changed since the last full or
            differential backup, which will be required for point-in-time restores, is to perform a
            log backup.287
            Chapter 8: Large or Full Transaction Log
            2. Full database backups do not truncate the transaction log. Only a log backup will
            cause the log file to be truncated. Without the latter, space in the log file will never be
            marked for reuse, and the log file will constantly grow in size.
            In order to perform a point-in-time recovery and control the size of the log, transaction
            log backups must be taken in conjunction with full and/or differential database backups.
            If you do discover that a lack of log backups is the cause of your log growth problems,
            the first thing to do is to verify that the database in question really does need the ability
            to recover to a point in time during a restore, and therefore needs to be operating in
            FULL recovery. If it doesn't, then switch to using the SIMPLE recovery model, where the
            inactive portion of the transaction log is automatically marked as reusable, at checkpoint.
            If the database does need to operate in the FULL recovery model, then start taking
            log backups. The frequency of the transaction log backups depends on a number of
            factors such as the frequency of data changes, and on SLAs for acceptable data loss in the
            event of a crash. Also, you should take steps to ensure that the log growth is controlled
            and predictable in future, as described in the Proper Log Management section, later in
            the chapter.
            Active transactions
            If the value returned for log_reuse_wait_desc is ACTIVE_TRANSACTION, then
            you are suffering from the second most common cause of a full or large transaction
            log in SQL Server: long-running or uncommitted transactions. As discussed in the
            Understanding log truncation section of this chapter, a VLF inside the transaction log
            can only be truncated when it contains no part of the active log, and if the database is
            using the FULL or BULK LOGGED recovery models, this truncation only occurs when the
            transactions contained in the VLF have been committed and backed up. Long-running
            transactions in a database delay truncation of the VLFs that contain the log records
            generated after the start of the transaction, including the log records generated by
            changes to data in the database by other sessions, even when those changes have been288
            Chapter 8: Large or Full Transaction Log
            committed. Additionally, the amount of space required by a long-running transaction
            will be increased by space reservations for "compensation log records," which are the log
            records that would be generated if the transaction were rolled back in the system. This
            reservation is required to ensure that the transaction can be reverted successfully without
            running out of log space during the rollback.
            Another common cause of the Active Transaction value for log_reuse_wait_desc
            is the presence of "orphaned" explicit transactions that somehow never got committed.
            Applications that allow for user input inside a transaction are especially prone to this kind
            of problem.
            Long-running transactions
            One of the most common operations that results in a long-running transaction, which
            also generates large numbers of log records in a database is archiving or purging of
            data from a database. Data retention tends to be an afterthought in database design,
            usually being considered after the database has been active for a period of time and is
            approaching the capacity limits of the available storage on a server.
            Usually, when the need to archive data arises, the first reaction is to remove the unneeded
            data from the database using a single DELETE statement, as shown in Listing 8.2.
            DELETE ExampleTable
            WHERE DateTimeCol < GETDATE() - 60
            Listing 8.2: Bulk data deletion.
            Depending on the number of rows that exist in the date range to be deleted, this can
            easily become a long-running transaction that will cause transaction log growth issues,
            even when the database is using the SIMPLE recovery model. The problem can be exacerbated by the presence of cascading FOREIGN KEY constraints or auditing triggers. If the
            table from which data is being deleted is referenced by other tables, using FOREIGN KEY289
            Chapter 8: Large or Full Transaction Log
            constraints that are designed to CASCADE ON DELETE, then details of the rows that are
            deleted through the cascading constraint will also be logged. If the table has a DELETE
            trigger on it, for auditing data changes, the operations being performed during the
            triggers execution will also be logged.
            To minimize the impact on the transaction log, the data purge operation should be
            broken down into a number of shorter, individual transactions. There are a number
            of ways to break a long-running transaction down into smaller batches. If cascading
            constraints or a DELETE trigger exist for a table, we can perform the DELETE operation
            inside of a loop, to delete one day of data at a time, as shown in Listing 8.3.
            DECLARE @StopDate DATETIME ,
            @PurgeDate DATETIME
            SELECT @PurgeDate = DATEADD(DAY, DATEDIFF(DAY, 0, MIN(DateTimeCol)), 0) ,
            @StopDate = DATEADD(DAY, DATEDIFF(DAY, 0, GETDATE()) - 60, 0)
            FROM ExampleTable
            WHILE @PurgeDate < @StopDate
            BEGIN
            DELETE ExampleTable
            WHERE DateTimeCol < @PurgeDate
            SELECT @PurgeDate = DATEADD(DAY, 1, @PurgeDate)
            END
            Listing 8.3: Breaking down data purges into smaller transactions.
            Using this model for purging data, the duration of each DELETE transaction is only the
            time required to delete a single day's data from the table, plus the time required for any
            triggers or cascading constraints to perform their operations. If the database uses the
            SIMPLE recovery model, the log records generated by each daily purge will be truncated
            the next time checkpoint occurs. If the database uses the FULL or BULK LOGGED recovery
            model, the log records generated by each daily purge will be truncated after the next log
            backup that occurs for the database, if no part of the active log exists inside the VLFs
            affected by the purge.290
            Chapter 8: Large or Full Transaction Log
            When cascading constraints or auditing triggers are not a factor in the process, a different
            method can be used to purge the data from the table while minimizing the transaction
            duration. Instead of performing a single day DELETE operation, which can affect more or
            less data depending on the number of rows that exist for a specific date, the TOP operator
            can be used inside the DELETE statement to limit the number of rows affected by each
            loop of the operation. By capturing into a variable the number of rows affected by the
            DELETE operation, using @@ROWCOUNT, the operation can continue to purge data from
            the table in small batches, until the value of @@ROWCOUNT is less than the number of rows
            specified in the TOP clause of the DELETE statement, as shown in Listing 8.4.
            This method only works when triggers and cascading constraints aren't being used
            because, when they are, the result of @@ROWCOUNT will not be the actual rows deleted
            from the base table, but instead the number of rows that are affected by the trigger
            execution or through enforcing the cascading constraint.
            DECLARE @Criteria DATETIME ,
            @RowCount INT
            SELECT @Criteria = GETDATE() - 60 ,
            @RowCount = 10000
            WHILE @RowCount = 10000
            BEGIN
            DELETE TOP ( 10000 )
            FROM ExampleTable
            WHERE DateTimeCol < @Criteria
            SELECT @RowCount = @@ROWCOUNT
            END
            Listing 8.4: Using the TOP operator inside the DELETE statement for data purges.
            These methods can be used in any edition of SQL Server 2000, 2005, and 2008 to
            minimize transaction duration during data purge operations.
            However, if the database is SQL Server 2005 or 2008 Enterprise Edition, and the data
            purging process will be run regularly, then an even better way to purge the data is to
            partition the table using a sliding window partition on the column being used to delete291
            Chapter 8: Large or Full Transaction Log
            the data. This will have even less impact on the transaction log, since the partition
            containing the data can be switched out of the table and truncated, which is a metadataonly operation.
            Managing archiving
            It is well outside the scope of this chapter to delve into full, automated archiving scheme, but a possible
            archiving process could involve partitioning, and duplicate schemas between tables, allowing a partition to be switched out of one table and into another one, minimizing the active portion of data in
            the main OLTP table, but reducing the archiving process to being metadata changes only. Kimberley
            Tripp has produced a detailed white paper called "Partitioned Tables and Indexes in SQL Server 2005,"
            which also covers the sliding window technique, (see http://msdn.microsoft.com/en-us/library/
            ms345146(v=sql.90).aspx).
            Uncommitted transactions
            By default, SQL Server wraps any data modification statement in an implicit transaction
            to ensure that, in the event of a failure, the changes already made at the point of failure
            can all be rolled back, returning the data to a consistent state. If the changes succeed, the
            implicit transaction is committed to the database. In contrast to implicit transactions,
            which occur automatically, explicit transactions are created in code to wrap multiple
            changes into a single transaction, ensuring that all the changes can be undone by issuing
            a ROLLBACK command, or persisted by issuing a COMMIT for the transaction.
            When used properly, explicit transactions can ensure that data modifications that span
            multiple tables complete successfully as a unit, or not at all. When used incorrectly,
            however, orphaned transactions can be left active in the database, preventing truncation
            of the transaction log, and so resulting in the transaction log growing or filling up. There
            are a number of cases that can result in an orphaned transaction in SQL Server, and it's
            beyond the scope of this chapter to investigate them in full detail.292
            Chapter 8: Large or Full Transaction Log
            However, some of the most common causes are:
            •	 application timeouts caused by a long-running transaction
            •	 incorrect error handling in TSQL or application code
            •	 failure during trigger execution
            •	 linked server failures resulting in orphaned distributed transactions
            •	 no corresponding COMMIT/ROLLBACK statement to a BEGIN TRANSACTION command.
            Once a transaction is created it will continue to remain active until a COMMIT or
            ROLLBACK statement is issued on the connection that created the transaction, or the
            connection disconnects from the SQL Server. It is critical that you understand this last
            point when troubleshooting orphaned transactions, since modern applications generally
            utilize connection pooling, keeping connections to the SQL Server in a pool for reuse
            by the application, even when the application code calls the Close() method on the
            connection. Even though the connection is reset before being added or returned to the
            application's connection pool, open transactions continue to exist in the database if they
            have not been properly terminated.
            Identifying the active transaction
            The fastest way to identify whether transaction log growth is being caused by
            an orphaned (or just long-running) transaction is to use DBCC OPENTRAN. This
            command can accept the database name as an input parameter in the format DBCC
            OPENTRAN(DatabaseName) where DatabaseName is the name of the database to check
            for open transactions.293
            Chapter 8: Large or Full Transaction Log
            If an active transaction exists in the database, this command will output information
            similar to the following:
            Transaction information for database 'TestDatabase'.
            Oldest active transaction:
            SPID (server process ID): 105
            UID (user ID) : -1
            Name : user_transaction
            LSN : (4212:44992:2)
            Start time : Mar 26 2010 2:37:31:907AM
            SID : 0x010500000000000515000000c5dc918918d95a068b7acf204f730000
            DBCC execution completed. If DBCC printed error messages, contact your system
            administrator.
            Only the oldest active transaction is reported by DBCC OPENTRAN, and the primary
            indicator of whether or not the active transaction is problematic is the Start Time.
            Generally, uncommitted transactions that become problematic with regard to transaction
            log growth have been open for a long period of time.
            The other important piece of information is the SPID (server process ID), which is the
            session that created the open transaction. We can use the SPID to determine whether the
            transaction is actually an orphaned transaction or just a long-running one, by querying
            the sysprocesses view (in SQL Server 2000) or the sys.dm_exec_sessions and
            sys.dm_exec_connections DMVs in SQL Server 2005 and 2008, as shown in Listing
            8.5. Note that the sysprocesses view is still available in SQL Server 2005 and 2008 for
            backwards compatibility.
            -- SQL 2000 sysprocess query
            SELECT spid ,
            status ,
            hostname ,
            program_name ,
            loginame ,
            login_time ,
            last_batch ,
            ( SELECT text294
            Chapter 8: Large or Full Transaction Log
            FROM ::
            fn_get_sql(sql_handle)
            ) AS [sql_text]
            FROM sysprocesses
            WHERE spid = <SPID>
            -- SQL 2005/2008 DMV query
            SELECT s.session_id ,
            s.status ,
            s.host_name ,
            s.program_name ,
            s.login_name ,
            s.login_time ,
            s.last_request_start_time ,
            s.last_request_end_time ,
            t.text
            FROM sys.dm_exec_sessions s
            JOIN sys.dm_exec_connections c ON s.session_id = c.session_id
            CROSS APPLY sys.dm_exec_sql_text(c.most_recent_sql_handle) t
            WHERE s.session_id = <SPID>
            Listing 8.5: Identifying orphaned or long-running transactions using the DMVs.
            If the SPID is in a runnable, running, or suspended status, then it is likely that the source
            of the problem is a long-running, rather than orphaned, transaction. However, further
            investigation of the command text will be needed to make the final determination. It is
            possible that an earlier transaction failed and the connection was reset, for use under
            connection pooling, and that the currently executing statement is not associated with the
            open transaction.
            In SQL Server 2005 and 2008, the sys.dm_tran_session_transactions and sys.
            dm_tran_database_transactions DMVs can be used to gather information specific
            to the open transaction including the transaction start time, number of log records used
            by the open transaction, as well as the bytes of log space used, as shown in Listing 8.6.295
            Chapter 8: Large or Full Transaction Log
            SELECT st.session_id ,
            st.is_user_transaction ,
            dt.database_transaction_begin_time ,
            dt.database_transaction_log_record_count ,
            dt.database_transaction_log_bytes_used
            FROM sys.dm_tran_session_transactions st
            JOIN sys.dm_tran_database_transactions dt
            ON st.transaction_id = dt.transaction_id
            AND dt.database_id = DB_ID('master')
            WHERE st.session_id = <SPID>
            Listing 8.6: Gathering information about the open transaction.
            If the open transaction was created before the last request start time, it is likely to be an
            orphaned transaction.
            Unless the application was specifically designed to check for, and handle, orphaned
            transactions, the only way to clear the transaction is to KILL the session which will cause
            the transaction to roll back as the connection terminates, allowing the space in the log
            be made available for reuse, during the next log backup. However, the ramifications of
            performing the rollback must be understood.
            Other possible causes of log growth
            In addition to those previously identified, there are a few other problems that may
            prevent reuse of space in the log, and so lead to excessive log growth.
            Why is my transaction log full?
            For further discussion on these issues, please see Gail Shaw's article, "Why is my transaction log full?" at
            http://www.sqlservercentral.com/articles/Transaction+Log/72488/.296
            Chapter 8: Large or Full Transaction Log
            Replication
            During transactional replication, it is the job of the log reader agent to read the transaction log, looking for log records that are associated with changes that need to be replicated to subscribers (i.e. are "pending replication"). Once the changes are replicated, it
            marks the log entry as "replicated." Slow or delayed log reader activity can lead to records
            being left as "pending replication" for long periods, during which time they will remain
            part of the active log, and so the parent VLF cannot be truncated. A similar problem exists
            for log records required by the Change Data Capture (CDC) feature.
            In either case, the log_reuse_wait_desc column of sys.databases will show
            REPLICATION as the root cause of the problem. The problem will also reveal itself in
            the form of bottlenecks in the throughput performance of the transaction log disk
            array, specifically, delayed read operations under concurrent write loads. As explained in
            Chapter 2, writes to the log file occur sequentially, but read operations associated with
            the log reader agent and log backups read the file sequentially as well. Having sequential
            reads and writes occurring at the same time can, depending on the level of activity in the
            system and the size of the active portion of the log, result in random I/O activity as the
            disk heads have to change position to read from the beginning of the active log and then
            write to the end of the active log. The disk latency counters, explained in Chapter 2, can
            be used to troubleshoot this type of problem.
            The first step in troubleshooting these REPLICATION wait issues is to verify that the log
            reader SQL Agent jobs are actually running. If they are not, attempt to start them. If this
            fails, you'll need to find out why.
            If the jobs are running but the REPLICATION waits persist, and the transaction log is
            growing rapidly, you need to find some way to get the relevant log entries marked as
            "replicated" so that space in their parent VLFs can be reused. Unfortunately, there is no
            perfect solution that will avoid side effects to replication or CDC in the environment, but
            you could try one of the solutions below.297
            Chapter 8: Large or Full Transaction Log
            •	 In the case of transactional replication, the sp_repldone command can be used
            to mark all of the log records currently waiting on the log reader to process them as
            processed, but this will require re-initialization of the subscribers, using a snapshot, to
            resynchronize the replication topology. With CDC, this command will not resolve the
            problem with transaction log growth.
            •	 Disabling CDC or replication and performing a manual resynchronization of the data.
            Once CDC or replication has been removed, the pending replication log records in the
            transaction log will no longer be pending and can be cleared by the next log backup
            in FULL or BULK LOGGED recovery or CHECKPOINT operation in SIMPLE recovery.
            However, the trade-off is that the environment will require manual synchronization of
            the data for CDC, or it will require re-initialization of the subscribers for replication, if
            these features are added back to the database.
            Remember that simply switching to the SIMPLE recovery model, in the hope of
            truncating the log, will not work since replication and CDC are both supported using
            SIMPLE recovery, and the log records will continue to be required until the log reader
            SQL Agent process harvests them.
            Snapshot Replication schema change issue
            There is a known issue with Snapshot Replication in SQL Server 2005 that causes log entries that are
            marked for replication of schema changes not to be unmarked when the changes are replicated. This
            problem is explained in the following blog post that also explains how to work around the issue by using
            sp_repldone: "Size of the Transaction Log Increasing and cannot be truncated or Shrinked due to
            Snapshot Replication" (http://blogs.msdn.com/b/sqlserverfaq/archive/2009/06/01/size-of-thetransaction-log-increasing-and-cannot-be-truncated-or-shrinked-due-to-snapshotreplication.aspx).298
            Chapter 8: Large or Full Transaction Log
            ACTIVE_BACKUP_OR_RESTORE
            When the log_reuse_wait_desc column shows ACTIVE_BACKUP_OR_RESTORE as
            the current wait description, a long-running full or differential backup of the database
            is the most likely cause of the log reuse problems. During a full or differential backup of
            the database, the backup process delays log truncation so that the active portion of the
            transaction log can be included as a part of the full backup. This allows changes made to
            database pages during the backup operation to be undone when the backup is restored
            WITH RECOVERY, to bring the database to a consistent state. If such waits are causing
            persistent problems, you'll need to investigate ways to optimize the backup process, such
            as by improving the performance of the backups (via backup compression) or improving
            the performance of the underlying disk I/O system.
            DATABASE_MIRRORING
            When the log_reuse_wait_desc column shows DATABASE_MIRRORING, as the
            current wait description, synchronous database mirroring operations may be the cause of
            the log reuse issues.
            In synchronous mirroring, transactions on the principal are only committed once their
            related log records have been transferred to the mirror database. If the connection to the
            mirror is slow or broken, or the mirroring session is suspended, then a large number of
            log records on the principal will remain part of the active log, preventing log space reuse,
            until they are copied over to the mirror.
            In such cases, I would first check the status of the mirroring session for the affected
            database(s). If they are not synchronizing correctly, then you will need to troubleshoot
            the cause of the failed connection between the principal and the mirror. One of the
            most common problems with database mirroring, when certificates are used to secure
            the endpoints, is the expiration of the certificates, requiring that they be recreated. A full
            discussion of troubleshooting mirroring connectivity problems is outside of the scope of
            this chapter but, unless the databases are properly synchronizing so that the log records299
            Chapter 8: Large or Full Transaction Log
            are being sent to the mirror, the active portion of the transaction log on the principal will
            continue to grow and not be able to be truncated without breaking the mirroring setup.
            If the transaction rate on the principal greatly exceeds the rate at which log records
            can be transferred to the mirror, then the log on the principal can grow rapidly. If the
            mirror server is being used for reporting, by creating snapshots, verify that the disk I/O
            configuration for the mirror is not saturated, by using the latency counters as explained
            in Chapter 2. If this is where the problem is, eliminating use of the mirror server for
            reporting may provide temporary relief of the problem. If the problem is strictly the
            sheer volume of transactions and the database is not running on SQL Server 2008 or
            higher, then upgrading may be able to resolve the problem due to the use of log stream
            compression in SQL Server 2008 and beyond.
            The best approach is to determine the cause of the mirroring issue and resolve it. For
            example, tuning operations that produce a significant number of log records, such as bulk
            loading data, or reorganizing indexes, may reduce the impact to the system overall during
            the operation.
            Handling a "Transaction Log Full" Error
            In the worst case, transaction log mismanagement or sudden, rapid, log growth can cause
            a transaction log to grow and grow and eventually eat up all available space on its drive.
            At this point it can grow no more, you'll encounter Error 9002, the transaction log full
            error, and the database will become read-only.
            Despite the urgency of this problem, it's important to react calmly, and avoid the sort
            of "spontaneous" solutions that are covered in the following section, Mismanagement
            or What Not To Do. Obviously the pressing concern is to allow SQL Server to continue
            to write to the log, by making more space available. The first port of call is to establish
            if the cause is a lack of log backups. Run the query in Listing 8.1 and if the value for the
            log_reuse_wait_desc column is Log Backup then this is the likely cause of the issue.300
            Chapter 8: Large or Full Transaction Log
            A query to the backupset table (http://msdn.microsoft.com/en-us/library/ms186299.
            aspx) in the MSDB database, as shown in Listing 8.7, will confirm whether or not log
            backups are being taken on the database, and when the last one was taken.
            USE msdb ;
            SELECT backup_set_id ,
            backup_start_date ,
            backup_finish_date ,
            backup_size ,
            recovery_model ,
            [type]
            FROM dbo.backupset
            WHERE database_name = 'DatabaseName'
            Listing 8.7: Determine when the last log backup was taken.
            In the type column, a D represents a database backup, L a log backup and I a differential
            backup. If log backups aren't being taken, or are being taken very infrequently, then your
            best course of action is to take a log backup (assuming the database is operating in FULL
            or BULK LOGGED recovery model). Hopefully, this will free up substantial space within the
            log and you can then implement an appropriate log backup scheme, and log file growth
            management strategy.
            If, for some reason, it is not possible to perform a log backup due to a lack of disk space,
            or the time it would take to perform a log backup exceeds the acceptable time to resolve
            the problem, then it might, depending on the disaster recovery policy for the database
            in question, be acceptable to force a truncation of the log by temporarily switching the
            database to the SIMPLE recovery model in order that inactive VLFs in the log can be
            truncated on CHECKPOINT. You can then switch the recovery model back to FULL and
            perform a new full database backup (or a differential backup, assuming a full backup
            was taken at some previous time) to restart the log chain for point-in-time recovery. Of
            course, you'll still need to investigate the problem fully, in order to make sure that the
            space isn't simply eaten up again.301
            Chapter 8: Large or Full Transaction Log
            Bear in mind also that, as discussed previously, if the problem preventing space reuse is
            anything other than Log Backup, then this technique won't work, since those records
            will simply remain part of the active log, preventing truncation.
            If a lack of log backups isn't the problem, or taking a log backup doesn't solve the
            problem, then investigating the cause will require a little more time. If you can easily and
            quickly make extra space on the log drive, by shifting off other files, or adding capacity to
            the current log drive, or adding an addition log file on a different disk array, then this will
            buy you the bit of breathing space you need to get the database out of read-only mode,
            and perform a log backup.
            If a log backup fails to free up space, you need to find out what is preventing space reuse
            in the log. Interrogate sys.databases (Listing 8.1) to find out if anything is preventing
            reuse of space in the log, and take appropriate action, as described throughout the
            previous Lack of log space reuse section.
            If this reveals nothing, you'll need to investigate further and find out which operations
            are causing the excessive logging that led to the log growth, as described in the Diagnosing
            a Runaway Transaction Log section.
            Ultimately, having resolved any space reuse issue, you may still be left with a log file
            that is consuming the vast majority of the space on the drive. As a one-off measure,
            i.e. assuming steps will be taken to ensure proper management of log growth in the
            future (see the Proper Log Management section, shortly), it is acceptable to use DBCC
            SHRINKFILE (see http://msdn.microsoft.com/en-us/library/ms189493.aspx) to
            reclaim the space used by a bloated transaction log file.
            You can either specify a target_size to which to shrink the log file, or you can specify
            0 (zero) as the target size and shrink the log to its smallest possible size, and then immediately resize it to a sensible size using ALTER DATABASE. The latter is the recommended
            way, as it minimizes fragmentation of the log file. This fragmentation issue is the main
            reason why you should never schedule regular DBCC SHRINKFILE tasks as a means of
            controlling the size of the log; this is discussed in more detail in the next section.302
            Chapter 8: Large or Full Transaction Log
            Mismanagement or What Not To Do
            Unfortunately, a quick search of the Internet for "Transaction Log Full" will return a
            number of forums threads, blog posts, and even articles published on seemingly reputable
            SQL Server sites, which recommend remedial action that is, frankly, dangerous. We'll
            cover a few of the more popular suggestions here.
            Detach database, delete log file
            The idea here is that you clear all users off the database, detach the database (or shut it
            down), delete the log file (or rename it) and then re-attach the database, causing a new log
            file to be created at whatever size is dictated by the model database. This is arguably the
            most appalling of all the terrible ways to handle a full transaction log. It can result in the
            database failing to start, leaving it in the RECOVERY_PENDING state.
            Depending on whether or not the database had been cleanly shut down at the time of the
            log deletion, the database may not be able to perform the UNDO and REDO operations that
            are a normal part of the database recovery process, because the transaction log is missing,
            and so can't return the database to a consistent state. When the log file is missing, and
            the database requires the transaction log to perform crash recovery, the database will fail
            to start up properly and the only recourse will be to restore the database from the most
            recent backup available, which will most likely result in data loss.303
            Chapter 8: Large or Full Transaction Log
            Creating, detaching, re-attaching, and fixing a suspect database
            Under specific circumstances, it may be possible to hack the existing database into a configuration that
            allows the transaction log to be rebuilt, although it may compromise the integrity of the data contained
            in the database. This type of operation is, at best, a last-ditch effort that may be used when there is
            absolutely no other way of recovering the database data, and it is not a recommended practice of the
            authors, technical editors, or anyone else involved in the authoring of this book. For an explanation of
            how to attempt hacking a database back into SQL Server where the transaction log file has been deleted,
            see Paul Randal's blog post, "Creating, detaching, re-attaching, and fixing a suspect database"
            (http://www.sqlskills.com/blogs/paul/post/TechEd-Demo-Creating-detaching-re-attaching-and-fixing-a-suspect-database.aspx).
            Forcing log file truncation
            In SQL Server 2000, BACKUP LOG WITH TRUNCATE_ONLY was a supported way of forcing
            SQL Server to truncate the transaction log, while the database was operating in the FULL
            or BULK LOGGED model, without actually making a backup copy of the contents of the
            log; the records in the truncated VLFs are simply discarded. So, unlike with a normal log
            backup, you're destroying your LSN chain and will only be able to restore to a point in
            time in any previous log backup files. Also, even though the database is set to FULL (or
            BULK LOGGED) recovery, it will actually, from that point on, operate in an auto-truncate
            mode, continuing to truncate inactive VLFs on checkpoint. In order to get the database
            operating in FULL recovery again, and restart the LSN chain, you'd need to perform a full
            (or differential) backup.
            This command was often used without people realizing the implications it had for
            disaster recovery, and it was deprecated in SQL Server 2005 and removed from SQL
            Server 2008. Unfortunately, an even more insidious variation of this technique, which
            continues to be supported, has crept up to take its place, and that is BACKUP LOG
            TO DISK='NUL', where NUL is a "virtual file" that discards any data that is written
            to it. The really nasty twist to this technique is that, unlike with BACKUP LOG WITH304
            Chapter 8: Large or Full Transaction Log
            TRUNCATE_ONLY, SQL Server is unaware that the log records have simply been discarded.
            As far as SQL Server is concerned, a log backup has been performed, the log records are
            safely stored in a backup file so the LSN chain is intact, and any inactive VLFs in the live
            log can safely be truncated. Any subsequent, conventional log backups will succeed but
            will be entirely useless from the point of view of disaster recovery since a log backup file
            is "missing" and so the database can only be restored to some point in time covered by the
            last standard log backup that was taken before BACKUP LOG TO DISK='NUL' was issued.
            Do not use either of these techniques. The right way to "force" log truncation is to
            temporarily switch the database into the SIMPLE recovery model, as discussed earlier.
            Scheduled shrinking of the transaction log
            As discussed in the Handling a "Transaction Log Full" error section, in rare circumstances
            where transaction log growth has occurred due to a lack of management, and where the
            log growth is currently being actively managed, using DBCC SHRINKFILE to reclaim the
            space used by the transaction log file is an acceptable operation.
            However, the transaction log should never be shrunk using DBCC SHRINKFILE, or a
            database maintenance plan step to shrink the database, as part of normal, scheduled
            maintenance operations. The reason for this is that every time you shrink the log, it will
            need to immediately grow again to store log records for subsequent transactions and
            every log. If auto-growth is being relied upon solely for transaction log growth (see the
            next section for a fuller discussion), excessive VLFs can accumulate in the log file and this
            log fragmentation will impact the performance of any process that needs to read the
            log file and, if fragmentation gets really bad, possibly even the performance of data
            modifications. Also, as discussed previously in the Sizing and growing the log section, the
            transaction log cannot take advantage of instant file initialization, so all log growths incur
            the cost to zero-byte the storage space being allocated.305
            Chapter 8: Large or Full Transaction Log
            The best practice for the transaction log file continues to be to size it appropriately up
            front so it does not have to grow under normal operations, and then to monitor its
            usage periodically to determine if the need to grow it manually occurs, allowing you to
            determine the appropriate growth size and determine the number and size of VLFs that
            will be added to the log file.
            Proper Log Management
            In the absence of any unexpected operations or problems that have resulted in unusual
            log growth (replication problems, uncommitted transactions, and so on, as discussed
            earlier), if the transaction log associated with a FULL recovery model database fills up,
            and is forced to grow, there are really only two causes:
            •	 the size of the log file was too small to support the volume of data changes that were
            occurring in the database
            •	 the frequency of log backups was insufficient to allow rapid reuse of space within the
            log file.
            The best thing to do, if you can't increase the frequency of the log backups by decreasing
            the amount of time between log backups, is to manually grow the log file to a size that
            prevents it from having to grow using auto-growth when under load, and then leave
            the log file that size. Having a large transaction log file that has been properly grown
            to minimize the number of VLFs is not a bad thing, even if the log file has free space a
            majority of the time.
            When configuring the initial size of the transaction log for a database, it is important to
            take into account the size of the largest table in the database, and whether or not index
            maintenance operations will be performed. As a rule of thumb, the transaction log should
            be sized to 1.5 times the size of the largest index or table in the database, to allow for
            logging requirements to rebuild the index under FULL recovery.306
            Chapter 8: Large or Full Transaction Log
            In addition to the initial sizing requirements, it is important to monitor and adjust the
            size of the transaction log periodically to fit the size of the database as it grows. There
            are a couple of problems with the auto-growth settings that a database will inherit from
            model, which is currently to grow in steps of 10% of the current transaction log size:
            •	 initially, when the log file is small, the incremental growth will be small, resulting in
            the creation of a large number of small VLFs in the log, causing the fragmentation
            issues discussed earlier
            •	 when the log file is very large, the growth increments will be correspondingly large;
            since the transaction log has to be zeroed out during initialization, large growth
            events can take time, and if the log can't be grown fast enough this can result in
            9002 (transaction log full) errors and even in the auto-growth timing out and
            being rolled back.
            The auto-growth settings should be configured explicitly to a fixed size that allows the log
            file to grow quickly, if necessary, while also minimizing the number of VLFs being added
            to the log file for each growth event.
            To minimize the chances of a timeout occurring during normal transaction log growth,
            you should measure how long it takes to grow the transaction log by a variety of set sizes
            while the database is operating under normal workload, and based on the current I/O
            subsystem configuration. In cases where the necessary zero-initialization performance
            characteristics are not known for a database, I recommend, as a general rule, a fixed autogrowth size of 512 MB.
            Ultimately, though, remember that auto-growth should be configured as a security net
            only, allowing the system to automatically grow the log file when you are unable to
            respond manually. Auto-growth is not a replacement for appropriate monitoring and
            management of the transaction log file size.307
            Chapter 8: Large or Full Transaction Log
            Summary
            The transaction log for a SQL Server database is critical to the operation of the database
            and the ability to minimize data loss in the event of a disaster. Proper management of log
            backups and sizing of the transaction log is crucial to managing a database in SQL Server.
            This chapter covered the most common causes for log growth in SQL Server, and how to
            properly handle those scenarios to minimize log usage, or troubleshoot the problem to
            prevent further issues with the transaction log.
        </main>
        <div id="topo">
            <a href="#" target="_self"><i class="material-symbols-outlined">stat_2</i></a>
        </div>

        <footer>
            <p>Criado por Agnaldo Mota</p>
        </footer>

        <script src="scripts/script.js"></script>
    </body>
</html>
